{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTD Challenge\n",
    "## Challenge task: \n",
    "\n",
    "__*Global Terrorism Database (GTD) is an open-source database including information on terrorist events around the world from 1970 through 2014. Some portion of the attacks have not been attributed to a particular terrorist group.*__\n",
    "\n",
    "\n",
    "__*Use attack type, weapons used, description of the attack, etc. to build a model that can predict what group may have been responsible for an incident.*__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain an accurate model and therefore the most accurate prediction, I have chosen to investigate the following learning models :\n",
    "\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "- Extreme Gradient Boosting\n",
    "- Support Vector Machine\n",
    "- Light Gradient Boosting\n",
    "- K-Nearest-Neighbours\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all packages used in analysis:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.svm import SVC \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell7450\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4,6,31,33,61,62,63,76,79,90,92,94,96,114,115,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventid</th>\n",
       "      <th>iyear</th>\n",
       "      <th>imonth</th>\n",
       "      <th>iday</th>\n",
       "      <th>approxdate</th>\n",
       "      <th>extended</th>\n",
       "      <th>resolution</th>\n",
       "      <th>country</th>\n",
       "      <th>country_txt</th>\n",
       "      <th>region</th>\n",
       "      <th>...</th>\n",
       "      <th>addnotes</th>\n",
       "      <th>scite1</th>\n",
       "      <th>scite2</th>\n",
       "      <th>scite3</th>\n",
       "      <th>dbsource</th>\n",
       "      <th>INT_LOG</th>\n",
       "      <th>INT_IDEO</th>\n",
       "      <th>INT_MISC</th>\n",
       "      <th>INT_ANY</th>\n",
       "      <th>related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197000000001</td>\n",
       "      <td>1970</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197000000002</td>\n",
       "      <td>1970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGIS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197001000001</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGIS</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197001000002</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>Greece</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGIS</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197001000003</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>Japan</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGIS</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        eventid  iyear  imonth  iday approxdate  extended resolution  country  \\\n",
       "0  197000000001   1970       7     2        NaN         0        NaN       58   \n",
       "1  197000000002   1970       0     0        NaN         0        NaN      130   \n",
       "2  197001000001   1970       1     0        NaN         0        NaN      160   \n",
       "3  197001000002   1970       1     0        NaN         0        NaN       78   \n",
       "4  197001000003   1970       1     0        NaN         0        NaN      101   \n",
       "\n",
       "          country_txt  region   ...    addnotes scite1 scite2  scite3  \\\n",
       "0  Dominican Republic       2   ...         NaN    NaN    NaN     NaN   \n",
       "1              Mexico       1   ...         NaN    NaN    NaN     NaN   \n",
       "2         Philippines       5   ...         NaN    NaN    NaN     NaN   \n",
       "3              Greece       8   ...         NaN    NaN    NaN     NaN   \n",
       "4               Japan       4   ...         NaN    NaN    NaN     NaN   \n",
       "\n",
       "   dbsource  INT_LOG  INT_IDEO INT_MISC INT_ANY  related  \n",
       "0      PGIS        0         0        0       0      NaN  \n",
       "1      PGIS        0         1        1       1      NaN  \n",
       "2      PGIS       -9        -9        1       1      NaN  \n",
       "3      PGIS       -9        -9        1       1      NaN  \n",
       "4      PGIS       -9        -9        1       1      NaN  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the dataset \n",
    "dataset=pd.read_csv('globalterrorismdb_0718dist.csv', encoding = \"ISO-8859-1\")\n",
    "dataset.head()\n",
    "#the error shown below does not affect our analysis as during \\\n",
    "#the preprocessing all the columns mentioned in error message are deleted from the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties of the dataframe: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181691 entries, 0 to 181690\n",
      "Columns: 135 entries, eventid to related\n",
      "dtypes: float64(55), int64(22), object(58)\n",
      "memory usage: 187.1+ MB\n",
      "(181691, 135)\n"
     ]
    }
   ],
   "source": [
    "print('Properties of the dataframe: ')\n",
    "dataset.info() \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are clearly 135 variables in our dataframe, we need to remove the unneccesary variables with regards to a predictive model. The columns include: the text version of another and subcategories of other more critical variables. Also, we must note that many are not available for all the events (i.e null)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing the columns which have a complementary translated numerical column we obtain a dataframe with  107  columns.\n"
     ]
    }
   ],
   "source": [
    "#We shall keep the numeric equivalent of the some of the columns, so these columns are already formatted \\\n",
    "# numerically to use for our classification techniques.\n",
    "\n",
    "dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_txt')))]\n",
    "\n",
    "print(\"After removing the columns which have a complementary translated numerical column we obtain a \\\n",
    "dataframe with \", len(dataset.columns.values), \" columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete column 'eventid' - unique and specific feature so needs to be deleted.\n",
    "\n",
    "del dataset['eventid'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing the columns that include missing entries (null) we obtain a dataframe with  23  columns.\n"
     ]
    }
   ],
   "source": [
    "#we must delete all the columns with missing values to strengthen our model\n",
    "dataset = dataset.dropna(axis=1)\n",
    "print(\"After removing the columns that include missing entries (null) we obtain a \\\n",
    "dataframe with \", len(dataset.columns.values), \" columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  3536  unique elements in the 'gname' (perpetrator) category in our  181691  element dataset (excluding 'Unknown' class). This means our target variable has high cardinality, which will indeed affect the speed and memory usage of our classifiers.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \", dataset['gname'].nunique()-1, \" unique elements in the 'gname' (perpetrator) category \\\n",
    "in our \", len(dataset['gname']), \" element dataset (excluding 'Unknown' class). This means our target variable has \\\n",
    "high cardinality, which will indeed affect the speed and memory usage of our classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns that are non-numeric and therefore need to be converted into a numeric form (aiding the classification models) are the following: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gname</th>\n",
       "      <th>dbsource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MANO-D</td>\n",
       "      <td>PGIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23rd of September Communist League</td>\n",
       "      <td>PGIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>PGIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>PGIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>PGIS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                gname dbsource\n",
       "0                              MANO-D     PGIS\n",
       "1  23rd of September Communist League     PGIS\n",
       "2                             Unknown     PGIS\n",
       "3                             Unknown     PGIS\n",
       "4                             Unknown     PGIS"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The columns that are non-numeric and therefore need to be converted into a numeric \\\n",
    "form (aiding the classification models) are the following: \")\n",
    "dataset.select_dtypes(exclude=['int64', 'int32', 'float64']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must note that both variables above are nominal, therefore we need not worry about the order of the elements when encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iyear</th>\n",
       "      <th>imonth</th>\n",
       "      <th>iday</th>\n",
       "      <th>extended</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>crit1</th>\n",
       "      <th>crit2</th>\n",
       "      <th>crit3</th>\n",
       "      <th>...</th>\n",
       "      <th>targtype1</th>\n",
       "      <th>gname</th>\n",
       "      <th>individual</th>\n",
       "      <th>weaptype1</th>\n",
       "      <th>property</th>\n",
       "      <th>INT_LOG</th>\n",
       "      <th>INT_IDEO</th>\n",
       "      <th>INT_MISC</th>\n",
       "      <th>INT_ANY</th>\n",
       "      <th>dbsource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>MANO-D</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>23rd of September Communist League</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iyear  imonth  iday  extended  country  region  vicinity  crit1  crit2  \\\n",
       "0   1970       7     2         0       58       2         0      1      1   \n",
       "1   1970       0     0         0      130       1         0      1      1   \n",
       "2   1970       1     0         0      160       5         0      1      1   \n",
       "3   1970       1     0         0       78       8         0      1      1   \n",
       "4   1970       1     0         0      101       4         0      1      1   \n",
       "\n",
       "   crit3    ...     targtype1                               gname  individual  \\\n",
       "0      1    ...            14                              MANO-D           0   \n",
       "1      1    ...             7  23rd of September Communist League           0   \n",
       "2      1    ...            10                             Unknown           0   \n",
       "3      1    ...             7                             Unknown           0   \n",
       "4      1    ...             7                             Unknown           0   \n",
       "\n",
       "   weaptype1 property  INT_LOG  INT_IDEO  INT_MISC  INT_ANY  dbsource  \n",
       "0         13        0        0         0         0        0        13  \n",
       "1         13        0        0         1         1        1        13  \n",
       "2         13        0       -9        -9         1        1        13  \n",
       "3          6        1       -9        -9         1        1        13  \n",
       "4          8        1       -9        -9         1        1        13  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First translate dbsource column using Label encoding:\n",
    "h=dataset['dbsource'].values #need to convert dataframe to array format to fit encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoderdb = label_encoder.fit(h)\n",
    "label_encoded_h = label_encoderdb.transform(h)\n",
    "#Delete the original 'dbsource' column from dataset and add the newly encoded 'dbsource' columns to original dataset:\n",
    "del dataset['dbsource']\n",
    "dataset = pd.concat([dataset,pd.DataFrame(label_encoded_h)],axis=1)\n",
    "dataset.rename(columns={0: 'dbsource'}, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'dbsource' column is nominal, therefore does not have any order to the elements. This means that we should in theory one-hot-encode the column, however, having tried this method, I found that keeping to the label encoded dbsource variable and building a model with this, improved the accuracy level by ~2-3%, therefore I shall not be choosing to one-hot-encode this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we continue to encode the 'gname' column - we must extract the 'Unknown' perpetrator rows from the dataset, \\\n",
    "# and split the dataset into features and targets:\n",
    "\n",
    "#let dfunknown be the dataset containing all the rows with 'unknown' perpetrators (ready to predict from, once models are built).\n",
    "dfunknown=dataset.loc[dataset['gname'] == 'Unknown'].reset_index(drop=True)\n",
    "\n",
    "#need to make sure that all rows with 'unknown' perpetrators are excluded when making trial and test sets, \\\n",
    "# so need to exclude from dataset.\n",
    "dataset=dataset[dataset.gname != 'Unknown'].reset_index(drop=True)\n",
    "\n",
    "#dfgname is the column 'gname' as a separate dataframe, ready as target values for trial and test sets.\n",
    "dfgname=dataset.pop('gname') \n",
    "#This leaves dataset as features dataframe for trial and test sets (w/o unknown targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now translate the gname category from names to integers using Label Encoding.\n",
    "\n",
    "y=dfgname.values #need to convert dataframe to array format to fit encoder\n",
    "label_encodergn = label_encoder.fit(y)\n",
    "label_encoded_y = label_encodergn.transform(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat the array of label encoded gname column into a data frame and name the column 'gname'.\n",
    "dfgname=pd.DataFrame(label_encoded_y)\n",
    "dfgname.columns=['gname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training data: 79127\n",
      "Number of observations in the test data: 19782\n"
     ]
    }
   ],
   "source": [
    "#Before encoding further, we can split it into trial and test data sets:\n",
    "\n",
    "#X=features , y = targets i.e gname column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label_encoded_y, test_size=0.2, random_state=123)\n",
    "\n",
    "#the following lengths should be the same as that of the targets:\n",
    "\n",
    "print('Number of observations in the training data:', len(X_train)) \n",
    "print('Number of observations in the test data:',len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high cardinality of the 'gname' target variable, I shall not be one-hot-encoding this variable as it would result in a memory failure error due to the addition of 3536 extra columns to the dataset. To solve this issue, I shall implement Binary encoder on this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To encode the gname (without 'unknown') column further, first need to reformat the arrays of target \\\n",
    "#training and testing sets built above into data frames ready to encode:\n",
    "\n",
    "y_traindf=pd.DataFrame(y_train)\n",
    "y_traindf.columns=['gname']\n",
    "\n",
    "y_testdf=pd.DataFrame(y_test)\n",
    "y_testdf.columns=['gname']\n",
    "\n",
    "y=dfgname\n",
    "X=dfgname\n",
    "\n",
    "# use binary encoding to encode categorical feature gname:\n",
    "enc = ce.BinaryEncoder(cols=['gname']).fit(X, y)\n",
    "\n",
    "# transform the dataset\n",
    "numeric_train = enc.transform(y_traindf)\n",
    "numeric_test = enc.transform(y_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have preprocessed our dataset we can begin the classification process, with Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "tree = DecisionTreeClassifier(criterion = 'gini').fit(X_train,numeric_train)\n",
    "\n",
    "#Predict the classes of new, unseen data\n",
    "prediction = tree.predict(X_test)\n",
    "\n",
    "#Check the accuracy\n",
    "print(\"The prediction accuracy is: \",tree.score(X_test,numeric_test)*100,\"%\")\n",
    " \n",
    "cm = confusion_matrix(numeric_test.values.argmax(axis=1), prediction.argmax(axis=1))\n",
    "print(\"confusion matrix: \",cm)\n",
    "\n",
    "print(\"10-Fold Cross Validation Score :\", 100 * np.mean(cross_val_score(tree, X_train, numeric_train, cv=10, n_jobs=-1)), \"%\")\n",
    "print(\"i.e according to accuracy_score, \\\n",
    "about \",accuracy_score(numeric_test, prediction, normalize=False), \" of the \", len(prediction), \"predictions were correct.\")\n",
    " \n",
    "\n",
    "# View a list of the features and their importance scores\n",
    "feature_list = list(X_train.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(tree.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:22} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "print(\"We can clearly see that from the importance scores above \",feature_importances[0],\" concerning the attacks \\\n",
    "was more important in classification than any other feature of the attack.\")\n",
    "\n",
    "#Before using our newly built tree to predict from our unknown data frame we need to remove the gname column of unknowns:\n",
    "df_features = dfunknown.drop(['gname'], axis=1)\n",
    "\n",
    "#now we can use the model to predict the unknown rows in df dataset:\n",
    "predictiontree = tree.predict(df_features)\n",
    "\n",
    "#want to translate these predictions into names of perpetrators\n",
    "s=pd.DataFrame(predictiontree) \n",
    "S=s.idxmax(axis=1)\n",
    "resultstree=label_encoder.inverse_transform(S)\n",
    "#translated (ohe-->label-->) results  \n",
    "#np.set_printoptions(threshold=np.inf)  #If one needs to see whole array of translated results\n",
    "print(\"Translated prediction (tree): \",resultstree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help reach a balance between bias and variance in our model we shall investigate further using Bagging. This is a technique used to reduce the variance of the predictions using the results of multiple classifiers modeled on different sub-samples of the same data set. This can be implemented using Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a forest:\n",
    "#Here I have tuned the parameters to have 500\n",
    "forest = RandomForestClassifier(n_estimators = 500, oob_score = True, n_jobs = -1,random_state =123, max_features=0.33).fit(X_train,numeric_train)\n",
    "prediction= forest.predict(X_test)\n",
    "print('Prediction (NUMERIC): ', prediction)\n",
    "print(\"The prediction accuracy is: \",forest.score(X_test,numeric_test)*100,\"%\")\n",
    "print(\"Out-of-bag Score :\",forest.oob_score_) #out-of-bag score\n",
    "\n",
    "cm = confusion_matrix(numeric_test.values.argmax(axis=1), prediction.argmax(axis=1))\n",
    "print(\"confusion matrix: \",cm)\n",
    "print(\"10-Fold Cross Validation Score :\", 100 * np.mean(cross_val_score(tree, X_train, numeric_train, cv=10, n_jobs=-1)), \"%\")\n",
    "print(\"i.e according to accuracy_score, \\\n",
    "about \",accuracy_score(numeric_test, prediction, normalize=False), \" of the \", len(prediction), \"predictions were correct.\")\n",
    "\n",
    "feature_list = list(X_train.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(forest.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "print(\"Feature importances according to the random forest:\")\n",
    "[print('Variable: {:22} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "print(\"We can clearly see that from the importance scores above \",feature_importances[0],\" concerning the attacks \\\n",
    "was still more important in classification than any other feature of the attack.\")\n",
    "\n",
    "#now we have constructed a forest we can use this to predict the unknown rows in df\n",
    "predictionforest= forest.predict(df_features)\n",
    "\n",
    "#want to translate these predictions into names of perpetrators\n",
    "s=pd.DataFrame(predictionforest) \n",
    "S=s.idxmax(axis=1)\n",
    "resultsforest=label_encoder.inverse_transform(S)\n",
    "#translated (ohe-->label-->) results  \n",
    "#np.set_printoptions(threshold=np.inf)  #If one needs to see whole array of translated results\n",
    "print(\"Translated prediction (forest): \",resultsforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the prediction accuracy has increased from approximately 75.5% to 77.5% when moving from the decision tree model to random forest model. I believe by tuning the forest's parameters, it can result in a higher level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extra Trees Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etclf = ExtraTreesClassifier(n_estimators=1000, max_depth=8, random_state=0, n_jobs = -1)\n",
    "etclf = etclf.fit(X_train, y_train)\n",
    "prediction = etclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The prediction accuracy is: \",etclf.score(X_test,y_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionforest= forest.predict(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to translate these predictions into names of perpetrators\n",
    "s=pd.DataFrame(predictionforest) \n",
    "S=s.idxmax(axis=1)\n",
    "resultsforest=label_encoder.inverse_transform(S)\n",
    "#translated (ohe-->label-->) results  \n",
    "#np.set_printoptions(threshold=np.inf)  #If one needs to see whole array of translated results\n",
    "print(\"Translated prediction (forest): \",resultsforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "_Due to several issues ranging from memory-failure and long running time, to other more technical failures, the following models have yet to be completed due to time constraints._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nc=df.nunique()\n",
    "#xg = xgb.XGBClassifier(objective ='multi:softmax', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                max_depth = 3, alpha = 10, n_estimators = 10, subsample =0.8, gamma=1)\n",
    "#xg.fit(X_train, y_train, early_stopping_rounds=10)\n",
    "#\n",
    "#predictions = xg.predict(X_test)\n",
    "#\n",
    "## evaluate predictions\n",
    "#accuracy = accuracy_score(y_test, predictions)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#\n",
    "##convert dataset to array\n",
    "#df_featuresarr=df_features.values\n",
    "#\n",
    "#y_predictionxg = model.predict(df_features)\n",
    "#predictionxg = [round(value) for value in y_predictionxg]\n",
    "#predictionxg=label_encoder.inverse_transform(predictionxg)\n",
    "#predictionxg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training a linear SVM classifier \n",
    "#svm_model_linear = SVC(kernel='linear').fit(X_train, y_train) \n",
    "#svm_pred = svm_model_linear.predict(X_test) \n",
    "#\n",
    "## model accuracy for X_test \n",
    "#accuracylin = svm_model_linear.score(X_test, y_test) \n",
    "#print(accuracylin)\n",
    "#\n",
    "## creating a confusion matrix \n",
    "#cmlin = confusion_matrix(y_test, svm_pred) \n",
    "#print(cmlin)\n",
    "#\n",
    "#print(confusion_matrix(y_test,svm_pred))  \n",
    "#print(classification_report(y_test,svm_pred))  \n",
    "#\n",
    "#svm_model_poly = SVC(kernel='poly', random_state=0, degree=8).fit(X_train, y_train) \n",
    "#svm_pred_poly = svm_model_poly.predict(X_test)\n",
    "#\n",
    "#accuracy_poly = svm_model_poly.score(X_test, y_test) \n",
    "#print(accuracy_poly)\n",
    "#\n",
    "#cm_poly = confusion_matrix(y_test, svm_pred_poly) \n",
    "#print(cm_poly)\n",
    "#\n",
    "#svm_model_gauss = SVC(kernel='rbf').fit(X_train, y_train) \n",
    "#svm_pred_gauss = svm_model_gauss.predict(X_test)\n",
    "#\n",
    "#accuracy_gauss = svm_model_gauss.score(X_test, y_test) \n",
    "#print(accuracy_gauss)\n",
    "# \n",
    "#cm_gauss = confusion_matrix(y_test, svm_pred_gauss) \n",
    "#print(cm_gauss)\n",
    "#\n",
    "#svm_model_sig = SVC(kernel='sigmoid').fit(X_train, y_train) \n",
    "#svm_pred_sig = svm_model_sig.predict(X_test)\n",
    "#\n",
    "#accuracy_sig = svm_model_sig.score(X_test, y_test) \n",
    "#print(accuracy_sig)\n",
    "#\n",
    "#cm_sig = confusion_matrix(y_test, svm_pred_sig) \n",
    "#print(cm_sig)\n",
    "#\n",
    "##Use the results above to gauge which of the kernels best suits the dataset and use this kernel to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Light GBM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "#train_data = lgb.Dataset(X_train, label=y_train, free_raw_data=True)\n",
    "#test_data = lgb.Dataset(X_test, label=y_test, reference=train_data, free_raw_data=True)\n",
    "\n",
    "#params = {'task': 'train',\n",
    "#    'boosting_type': 'gbdt',\n",
    "#    'objective': 'multiclass',\n",
    "#    'num_class':4000,\n",
    "#    'metric': 'multi_logloss',\n",
    "#    'learning_rate': 0.002296,\n",
    "#    'max_depth': 7,\n",
    "#    'num_leaves': 17,\n",
    "#    'feature_fraction': 0.4,\n",
    "#    'bagging_fraction': 0.6,\n",
    "#    'bagging_freq': 17}\n",
    "\n",
    "# evals_result = {}\n",
    "# gbm = lgb.train(params, train_data, num_boost_round=10000, nfold=3, shuffle=True, valid_sets=[train_data, test_data], \\\n",
    "#                 valid_names = ['train', 'valid'], evals_result=evals_result, \\\n",
    "#                 early_stopping_rounds=10, verbose_eval=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### K-Nearest-Neighbours Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()  \n",
    "#scaler.fit(X_train)\n",
    "## Apply transform to both the training set and the test set.\n",
    "#train = scaler.transform(X_train)\n",
    "#test = scaler.transform(X_test)\n",
    "#\n",
    "#pca = PCA(n_components=.95, random_state=123, svd_solver='full', whiten='True')\n",
    "#pca.fit(train)\n",
    "#\n",
    "#train = pca.transform(train)\n",
    "#test = pca.transform(test)\n",
    "#\n",
    "#pca.explained_variance_ratio_\n",
    "#\n",
    "#\n",
    "#scaler = preprocessing.MinMaxScaler() #found that adding this to preprocess increased the accuracy level by ~10%\n",
    "#X = scaler.fit_transform(X)\n",
    "#\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dataset, label_encoded_y, test_size=0.2, random_state=123)\n",
    "#\n",
    "#clf = KNeighborsClassifier(n_neighbors=3, p=1)\n",
    "#clf.fit(train, y_train)\n",
    "#print(clf)\n",
    "#y_pred = clf.predict(test)\n",
    "#\n",
    "#accuracy = accuracy_score(y_pred, y_test)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#\n",
    "#\n",
    "#print(confusion_matrix(y_test, y_pred))  \n",
    "#print(classification_report(y_test, y_pred)) \n",
    "#\n",
    "#error = []\n",
    "#\n",
    "## Calculating error for K values between 1 and 40\n",
    "#for i in range(1, 40):  \n",
    "#    knn = KNeighborsClassifier(n_neighbors=i, p=1)\n",
    "#    knn.fit(train, y_train)\n",
    "#    pred_i = knn.predict(test)\n",
    "#    error.append(np.mean(pred_i != y_test))\n",
    "#\n",
    "#plt.figure(figsize=(12, 6))  \n",
    "#plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n",
    "#         markerfacecolor='blue', markersize=10)\n",
    "#plt.title('Error Rate K Value')  \n",
    "#plt.xlabel('K Value')  \n",
    "#plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on the challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€¢**Ask yourself why would they have selected this problem for the challenge? What are some gotchas in this domain I should know about?**\n",
    "\n",
    "I believe that they would have selected this problem for the challenge because this problem has a lot of scope for further research using machine learning techniques with respect to predictive data modelling. In addition, this specific problem offers a large data set, with a dense set of fields and variables that the analyst will need to take into account and experiment with.\n",
    "Some of the â€˜gotchasâ€™ in this domain I think I should know about are the irregularities and its causes within the data (GTD Code booklet) and the areas from the data that could cause errors in the analysis had it been overlooked. The encoding of â€˜objectâ€™ features before passing it to a classifier is one example. The sheer size of the dataset (181691, 135) makes it more difficult to run it through a classifier whilst avoiding errors concerned with memory, along with the aspect of the algorithms merely taking too much time to complete. \n",
    "\n",
    "\n",
    "â€¢**What is the highest level of accuracy that others have achieved with this dataset or similar problems / datasets?**\n",
    "\n",
    "The highest level of accuracy that others have achieved with similar predictive modelling tasks using this dataset is exhibited in 'Predictive Modeling of Terrorist Attacks Using Machine Learning', International Journal of Pure and Applied Mathematics\n",
    "Volume 119 No. 15 2018, 49-61 (https://acadpubl.eu/hub/2018-119-15/4/630.pdf).\n",
    "\n",
    "\n",
    "â€¢**What types of visualizations will help me grasp the nature of the problem / data?**\n",
    "\n",
    "Decision tree and Random forest classifiers aided me in understanding the relative hierarchy of importance of features when calculating the perpetrator of the attacks.\n",
    "Because the target feature â€˜gnameâ€™, i.e. the perpetrators of the attacks, were objects along with it having a high cardinality of classes, I decided to steer away from plotting graphs and graphically visualising the relationship between the target set and the features of the data set. However it is brilliantly visualised in the following kaggle article by Laurenstc: https://www.kaggle.com/laurenstc/global-terrorism-analysis\n",
    "\n",
    "â€¢**What feature engineering might help improve the signal?**\n",
    "\n",
    "- Needed to discard the eventid column as keeping it would result in an overfitted model.\n",
    "\n",
    "- I decided to exclude the columns that had null values from the feature set as many of the classifiers used above would have failed to run. However, as a potential area for improvement, I could replace these null elements with separate predictions of those features using similar classification or regression techniques, thereby creating a larger and full data frame to use.\n",
    "\n",
    "- Using K-Fold cross validation on the test results (taking into account the original dataframe, including previously deleted features)- I could then gauge a better understanding on the error rate of the classifiers.\n",
    "\n",
    "â€¢**Which modelling techniques are good at capturing the types of relationships I see in this data?**\n",
    "\n",
    "I believe several incidents seem to be caused by more complex issues not represented in the dataframe like state of economy at the time and political and religious tensions, therefore it is difficult to realistically and accurately model this data. So maybe techniques like neural networks may prove to be useful when finding connections between the features and the target data set.\n",
    "\n",
    "â€¢**Now that I have a model, how can I be sure that I didn't introduce a bug in the code? If results are too good to be true, they probably are!**\n",
    "\n",
    "- Pruning the models to avoid overfitting\n",
    "- K-fold cross-validation\n",
    "- Experiment with hyperparameters of the models\n",
    "\n",
    "â€¢**What are some of the weaknesses of the model and how can the model be improved with additional work?**\n",
    "\n",
    "- In order to improve my models and increase its level of accuracy I would further tune the parameters for the classifiers and experiment with the combinations that provide the relatively higher level of accuracy. I would use cross validation to gain a better understanding of the accuracy measure.\n",
    "- Some of my models took too long to run, therefore to solve this issue, I could try to split the dataset into several dataframes using the 'iyear' category with respect to shorter time spans OR using 'country' (as this seems to be the most important feature from the few we have looked into). This will allow the algorithm to digest a smaller dataframe and therefore produce a prediction.  \n",
    "- Many of the original features were deleted due to null-elements, so instead of removing all the columns that included null elements â€“ I would build several models to help predict and therefore fill the respective elements to create a full (0-null value) dataset- thereby having a larger choice of features to investigate and exploit in terms of classifying the perpetrators.\n",
    "- I would build a neural network to see if I can classify the unknown perpetrators with a higher level of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
